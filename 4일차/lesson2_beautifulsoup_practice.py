"""
ì›¹ ìŠ¤í¬ë ˆì´í•‘ ì…ë¬¸ - 2êµì‹œ: BeautifulSoup ê¸°ë³¸ ì‚¬ìš©ë²• ì‹¤ìŠµ

ëŒ€ìƒ: íŒŒì´ì¬ì„ ì²˜ìŒ ë°°ìš°ëŠ” ì§ì¥ì¸
í•™ìŠµ ì‹œê°„: 30ë¶„
"""

import requests
from bs4 import BeautifulSoup


# ============================================================================
# ì„¹ì…˜ 1: BeautifulSoup ê¸°ë³¸ - HTML íŒŒì‹±í•˜ê¸°
# ============================================================================

def section1_basic_parsing():
    """ì„¹ì…˜ 1: ê¸°ë³¸ HTML íŒŒì‹±"""
    print("\n" + "="*70)
    print("ì„¹ì…˜ 1: BeautifulSoup ê¸°ë³¸ - HTML íŒŒì‹±í•˜ê¸°")
    print("="*70)
    
    print("\n[ì´ë¡ ]")
    print("BeautifulSoupì€ HTMLì„ íŒŒì‹±(êµ¬ì¡°ì ìœ¼ë¡œ í•´ì„)í•˜ì—¬")
    print("ì›í•˜ëŠ” ë°ì´í„°ë¥¼ ì‰½ê²Œ ì¶”ì¶œí•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.")
    
    print("\n[ì˜ˆì œ 1-1] ê°„ë‹¨í•œ HTML íŒŒì‹±")
    print("-" * 50)
    
    # HTML ë¬¸ìì—´
    html = """
    <html>
        <head>
            <title>ë‚´ ì²« ì›¹í˜ì´ì§€</title>
        </head>
        <body>
            <h1>ì•ˆë…•í•˜ì„¸ìš”!</h1>
            <p>BeautifulSoupë¥¼ ë°°ì›ë‹ˆë‹¤.</p>
        </body>
    </html>
    """
    
    # íŒŒì‹±
    soup = BeautifulSoup(html, 'html.parser')
    
    # ì˜ˆì˜ê²Œ ì¶œë ¥
    print("ğŸ“„ íŒŒì‹±ëœ HTML:")
    print(soup.prettify())
    
    print("\n[ì˜ˆì œ 1-2] íŠ¹ì • íƒœê·¸ ì°¾ê¸°")
    print("-" * 50)
    
    # title íƒœê·¸ ì°¾ê¸°
    title_tag = soup.find('title')
    print(f"ì œëª© íƒœê·¸: {title_tag}")
    print(f"ì œëª© í…ìŠ¤íŠ¸: {title_tag.text}")
    
    # h1 íƒœê·¸ ì°¾ê¸°
    h1_tag = soup.find('h1')
    print(f"\nH1 íƒœê·¸: {h1_tag}")
    print(f"H1 í…ìŠ¤íŠ¸: {h1_tag.text}")
    
    # p íƒœê·¸ ì°¾ê¸°
    p_tag = soup.find('p')
    print(f"\nP íƒœê·¸: {p_tag}")
    print(f"P í…ìŠ¤íŠ¸: {p_tag.text}")
    
    print("\n[ì„¤ëª…]")
    print("1. BeautifulSoup(html, 'html.parser')ë¡œ HTMLì„ íŒŒì‹±í•©ë‹ˆë‹¤")
    print("2. soup.find('íƒœê·¸ëª…')ìœ¼ë¡œ ì›í•˜ëŠ” íƒœê·¸ë¥¼ ì°¾ìŠµë‹ˆë‹¤")
    print("3. tag.textë¡œ íƒœê·¸ì˜ í…ìŠ¤íŠ¸ ë‚´ìš©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤")
    print("4. soup.prettify()ë¡œ HTMLì„ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")


# ============================================================================
# ì„¹ì…˜ 2: find()ì™€ find_all() - íƒœê·¸ ì°¾ê¸°
# ============================================================================

def section2_find_methods():
    """ì„¹ì…˜ 2: find()ì™€ find_all() ì‚¬ìš©ë²•"""
    print("\n" + "="*70)
    print("ì„¹ì…˜ 2: find()ì™€ find_all() - íƒœê·¸ ì°¾ê¸°")
    print("="*70)
    
    print("\n[ì´ë¡ ]")
    print("find() - ì²« ë²ˆì§¸ íƒœê·¸ í•˜ë‚˜ë§Œ ë°˜í™˜")
    print("find_all() - ì¡°ê±´ì— ë§ëŠ” ëª¨ë“  íƒœê·¸ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜")
    
    html = """
    <div class="news">
        <h2>ì²« ë²ˆì§¸ ë‰´ìŠ¤</h2>
        <p class="content">ì²« ë²ˆì§¸ ë‰´ìŠ¤ ë‚´ìš©</p>
        
        <h2>ë‘ ë²ˆì§¸ ë‰´ìŠ¤</h2>
        <p class="content">ë‘ ë²ˆì§¸ ë‰´ìŠ¤ ë‚´ìš©</p>
        
        <h2>ì„¸ ë²ˆì§¸ ë‰´ìŠ¤</h2>
        <p class="content">ì„¸ ë²ˆì§¸ ë‰´ìŠ¤ ë‚´ìš©</p>
    </div>
    """
    
    soup = BeautifulSoup(html, 'html.parser')
    
    print("\n[ì˜ˆì œ 2-1] find() - ì²« ë²ˆì§¸ ê²ƒë§Œ")
    print("-" * 50)
    
    # find()ëŠ” ì²« ë²ˆì§¸ ê²ƒë§Œ ë°˜í™˜
    first_h2 = soup.find('h2')
    print(f"ì²« ë²ˆì§¸ h2: {first_h2.text}")
    print(f"íƒ€ì…: {type(first_h2)}")  # bs4.element.Tag
    
    print("\n[ì˜ˆì œ 2-2] find_all() - ëª¨ë“  ê²ƒ")
    print("-" * 50)
    
    # find_all()ì€ ëª¨ë“  ê²ƒì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
    all_h2 = soup.find_all('h2')
    print(f"ì°¾ì€ h2 ê°œìˆ˜: {len(all_h2)}")
    print(f"íƒ€ì…: {type(all_h2)}")  # list
    
    print("\nëª¨ë“  h2 íƒœê·¸:")
    for i, h2 in enumerate(all_h2, 1):
        print(f"{i}. {h2.text}")
    
    print("\n[ì˜ˆì œ 2-3] í´ë˜ìŠ¤ë¡œ ì°¾ê¸°")
    print("-" * 50)
    
    # class ì†ì„±ìœ¼ë¡œ ì°¾ê¸° (ì£¼ì˜: class_ ì‚¬ìš©)
    content_tags = soup.find_all('p', class_='content')
    print(f"class='content'ì¸ p íƒœê·¸ ê°œìˆ˜: {len(content_tags)}")
    
    for i, p in enumerate(content_tags, 1):
        print(f"{i}. {p.text}")
    
    print("\n[ì˜ˆì œ 2-4] ì—¬ëŸ¬ ì¡°ê±´ìœ¼ë¡œ ì°¾ê¸°")
    print("-" * 50)
    
    html2 = """
    <a href="/news/1" class="news-link">ë‰´ìŠ¤ 1</a>
    <a href="/news/2" class="news-link">ë‰´ìŠ¤ 2</a>
    <a href="/notice/1" class="notice-link">ê³µì§€ 1</a>
    """
    
    soup2 = BeautifulSoup(html2, 'html.parser')
    
    # íƒœê·¸ + í´ë˜ìŠ¤
    news_links = soup2.find_all('a', class_='news-link')
    print(f"news-link í´ë˜ìŠ¤ë¥¼ ê°€ì§„ a íƒœê·¸: {len(news_links)}ê°œ")
    
    for link in news_links:
        print(f"  - {link.text}")
    
    print("\n[ì„¤ëª…]")
    print("1. find('íƒœê·¸')ëŠ” ì²« ë²ˆì§¸ ê²ƒë§Œ ë°˜í™˜ (Tag ê°ì²´)")
    print("2. find_all('íƒœê·¸')ëŠ” ëª¨ë“  ê²ƒì„ ë°˜í™˜ (ë¦¬ìŠ¤íŠ¸)")
    print("3. classë¡œ ì°¾ì„ ë•ŒëŠ” class_ (ì–¸ë”ìŠ¤ì½”ì–´) ì‚¬ìš©")
    print("4. ì—¬ëŸ¬ ì¡°ê±´ì„ ì¡°í•©í•˜ì—¬ ì •í™•í•˜ê²Œ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤")


# ============================================================================
# ì„¹ì…˜ 3: ë°ì´í„° ì¶”ì¶œ - í…ìŠ¤íŠ¸ì™€ ì†ì„±
# ============================================================================

def section3_extract_data():
    """ì„¹ì…˜ 3: í…ìŠ¤íŠ¸ì™€ ì†ì„± ì¶”ì¶œí•˜ê¸°"""
    print("\n" + "="*70)
    print("ì„¹ì…˜ 3: ë°ì´í„° ì¶”ì¶œ - í…ìŠ¤íŠ¸ì™€ ì†ì„±")
    print("="*70)
    
    print("\n[ì´ë¡ ]")
    print("íƒœê·¸ì—ì„œ ì¶”ì¶œí•  ìˆ˜ ìˆëŠ” ê²ƒ:")
    print("1. í…ìŠ¤íŠ¸: tag.text ë˜ëŠ” tag.get_text()")
    print("2. ì†ì„±: tag['ì†ì„±ëª…'] ë˜ëŠ” tag.get('ì†ì„±ëª…')")
    
    print("\n[ì˜ˆì œ 3-1] í…ìŠ¤íŠ¸ ì¶”ì¶œ")
    print("-" * 50)
    
    html = """
    <div class="article">
        <h2>   ì œëª©ì…ë‹ˆë‹¤   </h2>
        <p>ë³¸ë¬¸ ë‚´ìš©ì…ë‹ˆë‹¤.</p>
    </div>
    """
    
    soup = BeautifulSoup(html, 'html.parser')
    
    h2 = soup.find('h2')
    print(f"ì›ë³¸ í…ìŠ¤íŠ¸: '{h2.text}'")
    print(f"ê³µë°± ì œê±°: '{h2.text.strip()}'")
    
    # get_text() ì‚¬ìš©
    print(f"\nget_text(): '{h2.get_text()}'")
    print(f"get_text(strip=True): '{h2.get_text(strip=True)}'")
    
    print("\n[ì˜ˆì œ 3-2] ë§í¬(href) ì¶”ì¶œ")
    print("-" * 50)
    
    html_links = """
    <div>
        <a href="https://www.python.org">Python ê³µì‹ ì‚¬ì´íŠ¸</a>
        <a href="https://www.google.com">êµ¬ê¸€</a>
        <a href="/news/123">ë‰´ìŠ¤ ê¸°ì‚¬</a>
    </div>
    """
    
    soup = BeautifulSoup(html_links, 'html.parser')
    
    links = soup.find_all('a')
    print(f"ì°¾ì€ ë§í¬ ê°œìˆ˜: {len(links)}\n")
    
    for i, link in enumerate(links, 1):
        text = link.text
        url = link['href']  # ì†ì„± ì¶”ì¶œ
        print(f"{i}. í…ìŠ¤íŠ¸: {text}")
        print(f"   URL: {url}\n")
    
    print("[ì˜ˆì œ 3-3] ì´ë¯¸ì§€(src, alt) ì¶”ì¶œ")
    print("-" * 50)
    
    html_imgs = """
    <div>
        <img src="/images/logo.png" alt="ë¡œê³ ">
        <img src="/images/banner.jpg" alt="ë°°ë„ˆ">
    </div>
    """
    
    soup = BeautifulSoup(html_imgs, 'html.parser')
    
    images = soup.find_all('img')
    
    for i, img in enumerate(images, 1):
        src = img['src']
        alt = img['alt']
        print(f"{i}. ì´ë¯¸ì§€ ê²½ë¡œ: {src}")
        print(f"   ëŒ€ì²´ í…ìŠ¤íŠ¸: {alt}\n")
    
    print("[ì˜ˆì œ 3-4] ì•ˆì „í•˜ê²Œ ì†ì„± ì¶”ì¶œ (get ë©”ì„œë“œ)")
    print("-" * 50)
    
    html_safe = '<a>ë§í¬ì¸ë° hrefê°€ ì—†ìŒ</a>'
    soup = BeautifulSoup(html_safe, 'html.parser')
    
    link = soup.find('a')
    
    # ìœ„í—˜: KeyError ë°œìƒ ê°€ëŠ¥
    # href = link['href']  # âŒ
    
    # ì•ˆì „: get() ì‚¬ìš©
    href = link.get('href')
    if href:
        print(f"URL: {href}")
    else:
        print("âœ… href ì†ì„±ì´ ì—†ìŠµë‹ˆë‹¤ (ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)")
    
    # ê¸°ë³¸ê°’ ì„¤ì •
    href = link.get('href', 'ë§í¬ ì—†ìŒ')
    print(f"ê¸°ë³¸ê°’ ì‚¬ìš©: {href}")
    
    print("\n[ì„¤ëª…]")
    print("1. tag.text ë˜ëŠ” tag.get_text()ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤")
    print("2. strip=Trueë¡œ ì•ë’¤ ê³µë°±ì„ ì œê±°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
    print("3. tag['ì†ì„±ëª…']ìœ¼ë¡œ ì†ì„± ê°’ì„ ì¶”ì¶œí•©ë‹ˆë‹¤")
    print("4. tag.get('ì†ì„±ëª…')ì„ ì‚¬ìš©í•˜ë©´ ì†ì„±ì´ ì—†ì–´ë„ ì•ˆì „í•©ë‹ˆë‹¤")


# ============================================================================
# ì„¹ì…˜ 4: HTML êµ¬ì¡° íƒìƒ‰ - ë¶€ëª¨, ìì‹, í˜•ì œ
# ============================================================================

def section4_navigate_tree():
    """ì„¹ì…˜ 4: HTML êµ¬ì¡° íƒìƒ‰í•˜ê¸°"""
    print("\n" + "="*70)
    print("ì„¹ì…˜ 4: HTML êµ¬ì¡° íƒìƒ‰ - ë¶€ëª¨, ìì‹, í˜•ì œ")
    print("="*70)
    
    print("\n[ì´ë¡ ]")
    print("HTMLì€ íŠ¸ë¦¬ êµ¬ì¡°ì…ë‹ˆë‹¤. íƒœê·¸ ê°„ì˜ ê´€ê³„:")
    print("- ë¶€ëª¨(parent): ìƒìœ„ íƒœê·¸")
    print("- ìì‹(children): í•˜ìœ„ íƒœê·¸")
    print("- í˜•ì œ(siblings): ê°™ì€ ë ˆë²¨ì˜ íƒœê·¸")
    
    print("\n[ì˜ˆì œ 4-1] ë¶€ëª¨ ìš”ì†Œ ì ‘ê·¼")
    print("-" * 50)
    
    html = """
    <div class="container">
        <h1 id="title">ì œëª©</h1>
    </div>
    """
    
    soup = BeautifulSoup(html, 'html.parser')
    
    h1 = soup.find('h1')
    print(f"í˜„ì¬ íƒœê·¸: {h1.name}")
    print(f"í˜„ì¬ íƒœê·¸ id: {h1.get('id')}")
    
    # ë¶€ëª¨ ì ‘ê·¼
    parent = h1.parent
    print(f"\në¶€ëª¨ íƒœê·¸: {parent.name}")
    print(f"ë¶€ëª¨ í´ë˜ìŠ¤: {parent.get('class')}")
    
    print("\n[ì˜ˆì œ 4-2] ìì‹ ìš”ì†Œ ì ‘ê·¼")
    print("-" * 50)
    
    html2 = """
    <ul class="menu">
        <li>ë©”ë‰´ 1</li>
        <li>ë©”ë‰´ 2</li>
        <li>ë©”ë‰´ 3</li>
    </ul>
    """
    
    soup = BeautifulSoup(html2, 'html.parser')
    
    ul = soup.find('ul')
    
    # ëª¨ë“  li ìì‹ ì°¾ê¸°
    children = ul.find_all('li')
    print(f"ìì‹(li) ê°œìˆ˜: {len(children)}\n")
    
    for i, child in enumerate(children, 1):
        print(f"{i}. {child.text}")
    
    print("\n[ì˜ˆì œ 4-3] í˜•ì œ ìš”ì†Œ ì ‘ê·¼")
    print("-" * 50)
    
    html3 = """
    <div>
        <h1>ì œëª©</h1>
        <p>ì²« ë²ˆì§¸ ë¬¸ë‹¨</p>
        <p>ë‘ ë²ˆì§¸ ë¬¸ë‹¨</p>
        <p>ì„¸ ë²ˆì§¸ ë¬¸ë‹¨</p>
    </div>
    """
    
    soup = BeautifulSoup(html3, 'html.parser')
    
    h1 = soup.find('h1')
    print(f"í˜„ì¬ ìœ„ì¹˜: {h1.text}\n")
    
    # ë‹¤ìŒ í˜•ì œ
    next_sib = h1.find_next_sibling()
    print(f"ë‹¤ìŒ í˜•ì œ: {next_sib.text}")
    
    # ëª¨ë“  ë‹¤ìŒ í˜•ì œë“¤
    all_next = h1.find_next_siblings()
    print(f"\nëª¨ë“  ë‹¤ìŒ í˜•ì œ ({len(all_next)}ê°œ):")
    for i, sib in enumerate(all_next, 1):
        print(f"{i}. {sib.text}")
    
    print("\n[ì„¤ëª…]")
    print("1. tag.parentë¡œ ë¶€ëª¨ ìš”ì†Œì— ì ‘ê·¼í•©ë‹ˆë‹¤")
    print("2. tag.find_all()ì´ë‚˜ tag.childrenìœ¼ë¡œ ìì‹ì— ì ‘ê·¼í•©ë‹ˆë‹¤")
    print("3. tag.find_next_sibling()ìœ¼ë¡œ ë‹¤ìŒ í˜•ì œì— ì ‘ê·¼í•©ë‹ˆë‹¤")
    print("4. tag.find_next_siblings()ë¡œ ëª¨ë“  ë‹¤ìŒ í˜•ì œë¥¼ ì°¾ìŠµë‹ˆë‹¤")


# ============================================================================
# ì„¹ì…˜ 5: ì‹¤ì „ ì˜ˆì œ - ì›¹í˜ì´ì§€ì—ì„œ ë°ì´í„° ì¶”ì¶œ
# ============================================================================

def section5_real_example():
    """ì„¹ì…˜ 5: ì‹¤ì „ - ëª¨ë“  ë§í¬ ì¶”ì¶œí•˜ê¸°"""
    print("\n" + "="*70)
    print("ì„¹ì…˜ 5: ì‹¤ì „ ì˜ˆì œ - ì›¹í˜ì´ì§€ì—ì„œ ëª¨ë“  ë§í¬ ì¶”ì¶œ")
    print("="*70)
    
    print("\n[ì´ë¡ ]")
    print("ì‹¤ì œ ì›¹í˜ì´ì§€ì—ì„œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ëŠ” ì „ì²´ í”„ë¡œì„¸ìŠ¤:")
    print("1. requestsë¡œ HTML ê°€ì ¸ì˜¤ê¸°")
    print("2. BeautifulSoupìœ¼ë¡œ íŒŒì‹±")
    print("3. find/find_allë¡œ ì›í•˜ëŠ” ìš”ì†Œ ì°¾ê¸°")
    print("4. í…ìŠ¤íŠ¸ì™€ ì†ì„± ì¶”ì¶œ")
    
    print("\n[ì˜ˆì œ 5-1] ì™„ì „í•œ ì‹¤ì „ ì˜ˆì œ")
    print("-" * 50)
    
    def extract_all_links(url):
        """ì›¹í˜ì´ì§€ì˜ ëª¨ë“  ë§í¬ ì¶”ì¶œ"""
        try:
            print(f"ğŸ“¡ {url}ì— ì ‘ì† ì¤‘...")
            
            # 1. HTML ê°€ì ¸ì˜¤ê¸°
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(url, headers=headers, timeout=5)
            response.raise_for_status()
            
            print(f"âœ… ì„±ê³µ! (ìƒíƒœ ì½”ë“œ: {response.status_code})")
            
            # 2. íŒŒì‹±
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # 3. ëª¨ë“  a íƒœê·¸ ì°¾ê¸°
            links = soup.find_all('a')
            print(f"ğŸ“Š ì°¾ì€ ë§í¬ ê°œìˆ˜: {len(links)}\n")
            
            # 4. ë°ì´í„° ì¶”ì¶œ ë° ì •ë¦¬
            results = []
            for link in links:
                href = link.get('href')
                text = link.text.strip()
                
                if href:  # hrefê°€ ìˆëŠ” ê²½ìš°ë§Œ
                    results.append({
                        'text': text if text else '(í…ìŠ¤íŠ¸ ì—†ìŒ)',
                        'url': href
                    })
            
            return results
            
        except requests.exceptions.Timeout:
            print(f"â±ï¸ íƒ€ì„ì•„ì›ƒ: {url}")
            return []
        except requests.exceptions.HTTPError as e:
            print(f"âŒ HTTP ì˜¤ë¥˜: {e}")
            return []
        except Exception as e:
            print(f"âš ï¸ ì˜¤ë¥˜: {e}")
            return []
    
    # í…ŒìŠ¤íŠ¸ (ì¸í„°ë„· ì—°ê²° í•„ìš”)
    url = 'http://example.com'
    links = extract_all_links(url)
    
    if links:
        print(f"ì¶”ì¶œ ê²°ê³¼ (ìƒìœ„ 5ê°œ):")
        print("-" * 50)
        for i, link in enumerate(links[:5], 1):
            print(f"{i}. {link['text']}")
            print(f"   â†’ {link['url']}\n")
    
    print("\n[ì„¤ëª…]")
    print("ì´ í•¨ìˆ˜ëŠ” ì‹¤ë¬´ì—ì„œ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
    print("1. User-Agent ì„¤ì •ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ì ‘ê·¼")
    print("2. ì˜ˆì™¸ ì²˜ë¦¬ë¡œ ì˜¤ë¥˜ì— ëŒ€ë¹„")
    print("3. ë”•ì…”ë„ˆë¦¬ë¡œ êµ¬ì¡°í™”ëœ ë°ì´í„° ë°˜í™˜")
    print("4. ë¹ˆ í…ìŠ¤íŠ¸ë‚˜ ì—†ëŠ” href ì²˜ë¦¬")


# ============================================================================
# ì‹¤ìŠµ ë¯¸ì…˜
# ============================================================================

def practice_missions():
    """ì‹¤ìŠµ ë¯¸ì…˜ - ì§ì ‘ í•´ë³´ê¸°"""
    print("\n" + "="*70)
    print("ğŸ’ª ì‹¤ìŠµ ë¯¸ì…˜")
    print("="*70)
    
    print("\n[ë¯¸ì…˜ 1] HTMLì—ì„œ íŠ¹ì • íƒœê·¸ì˜ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ê¸°")
    print("-" * 50)
    print("ë‹¤ìŒ HTMLì—ì„œ ëª¨ë“  <h2> íƒœê·¸ì˜ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.")
    print()
    print("HTML:")
    print("""
    <div>
        <h2>ì œëª© 1</h2>
        <p>ë‚´ìš© 1</p>
        <h2>ì œëª© 2</h2>
        <p>ë‚´ìš© 2</p>
        <h2>ì œëª© 3</h2>
        <p>ë‚´ìš© 3</p>
    </div>
    """)
    print("\níŒíŠ¸: find_all('h2')ì™€ ë°˜ë³µë¬¸ ì‚¬ìš©")
    
    print("\n[ë¯¸ì…˜ 2] ë§í¬ì™€ í…ìŠ¤íŠ¸ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ì €ì¥í•˜ê¸°")
    print("-" * 50)
    print("ë‹¤ìŒ HTMLì—ì„œ ëª¨ë“  ë§í¬ë¥¼ ì¶”ì¶œí•˜ì—¬")
    print("{'text': 'í…ìŠ¤íŠ¸', 'url': 'URL'} í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“œì„¸ìš”.")
    print()
    print("HTML:")
    print("""
    <nav>
        <a href="/home">í™ˆ</a>
        <a href="/about">ì†Œê°œ</a>
        <a href="/contact">ì—°ë½ì²˜</a>
    </nav>
    """)
    print("\níŒíŠ¸: find_all('a'), link.text, link['href']")
    
    print("\nğŸ’¡ ì§ì ‘ ì½”ë“œë¥¼ ì‘ì„±í•´ë³´ì„¸ìš”!")


# ============================================================================
# í€´ì¦ˆ
# ============================================================================

def quiz():
    """í€´ì¦ˆ - í•™ìŠµ ë‚´ìš© í™•ì¸"""
    print("\n" + "="*70)
    print("ğŸ“ í€´ì¦ˆ")
    print("="*70)
    
    questions = [
        {
            "question": "1. BeautifulSoupìœ¼ë¡œ HTMLì„ íŒŒì‹±í•  ë•Œ ì‚¬ìš©í•˜ëŠ” ê¸°ë³¸ íŒŒì„œëŠ”?",
            "options": ["a) lxml", "b) html.parser", "c) xml.parser", "d) json.parser"],
            "answer": "b"
        },
        {
            "question": "2. ì²« ë²ˆì§¸ h1 íƒœê·¸ë¥¼ ì°¾ëŠ” ì˜¬ë°”ë¥¸ ì½”ë“œëŠ”?",
            "options": ["a) soup.find('h1')", "b) soup.find_all('h1')[0]", "c) soup.h1", "d) ëª¨ë‘ ê°€ëŠ¥"],
            "answer": "d"
        },
        {
            "question": "3. íƒœê·¸ì˜ href ì†ì„±ì„ ì•ˆì „í•˜ê²Œ ì¶”ì¶œí•˜ëŠ” ë°©ë²•ì€?",
            "options": ["a) tag['href']", "b) tag.href", "c) tag.get('href')", "d) tag.attribute('href')"],
            "answer": "c"
        }
    ]
    
    for i, q in enumerate(questions, 1):
        print(f"\n{q['question']}")
        for option in q['options']:
            print(f"  {option}")
    
    print("\n(ì •ë‹µì€ í•˜ë‹¨ì˜ 'ì •ë‹µ ë° í•´ì„¤'ì—ì„œ í™•ì¸í•˜ì„¸ìš”)")
    print("="*70)


# ============================================================================
# ì •ë‹µ ë° í•´ì„¤
# ============================================================================

def show_answers():
    """ì •ë‹µ ë° í•´ì„¤"""
    print("\n" + "="*70)
    print("âœ… ì •ë‹µ ë° í•´ì„¤")
    print("="*70)
    
    print("""
<details>
<summary>ì •ë‹µ ë³´ê¸°</summary>

1. ì •ë‹µ: b) html.parser
   í•´ì„¤: html.parserëŠ” Python ê¸°ë³¸ ì œê³µ íŒŒì„œë¡œ ë³„ë„ ì„¤ì¹˜ê°€ í•„ìš” ì—†ìŠµë‹ˆë‹¤.
         ì´ˆë³´ìì—ê²Œ ê°€ì¥ ê¶Œì¥ë˜ëŠ” íŒŒì„œì…ë‹ˆë‹¤.
         lxmlì€ ë” ë¹ ë¥´ì§€ë§Œ ë³„ë„ ì„¤ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.

2. ì •ë‹µ: d) ëª¨ë‘ ê°€ëŠ¥
   í•´ì„¤: soup.find('h1'), soup.find_all('h1')[0], soup.h1 
         ëª¨ë‘ ì²« ë²ˆì§¸ h1 íƒœê·¸ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
         í•˜ì§€ë§Œ find()ê°€ ê°€ì¥ ëª…ì‹œì ì´ê³  ì½ê¸° ì‰½ìŠµë‹ˆë‹¤.

3. ì •ë‹µ: c) tag.get('href')
   í•´ì„¤: get() ë©”ì„œë“œëŠ” ì†ì„±ì´ ì—†ì–´ë„ Noneì„ ë°˜í™˜í•˜ë¯€ë¡œ ì•ˆì „í•©ë‹ˆë‹¤.
         tag['href']ëŠ” ì†ì„±ì´ ì—†ìœ¼ë©´ KeyErrorê°€ ë°œìƒí•©ë‹ˆë‹¤.
         get('ì†ì„±', 'ê¸°ë³¸ê°’')ìœ¼ë¡œ ê¸°ë³¸ê°’ì„ ì§€ì •í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

</details>
    """)


# ============================================================================
# ì¢…í•© ì‹¤ìŠµ ì˜ˆì œ
# ============================================================================

def comprehensive_example():
    """ì¢…í•© ì‹¤ìŠµ: ë‰´ìŠ¤ ì œëª©ê³¼ ë§í¬ ì¶”ì¶œ"""
    print("\n" + "="*70)
    print("ğŸš€ ì¢…í•© ì‹¤ìŠµ: ë‰´ìŠ¤ ì œëª©ê³¼ ë§í¬ ì¶”ì¶œ")
    print("="*70)
    
    print("\n[ì‹¤ë¬´ ìŠ¤íƒ€ì¼ì˜ ì™„ì „í•œ ì˜ˆì œ]")
    print("-" * 50)
    
    # ìƒ˜í”Œ HTML (ì‹¤ì œ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ êµ¬ì¡° ëª¨ë°©)
    sample_html = """
    <html>
    <body>
        <div class="news-list">
            <div class="news-item">
                <a href="/news/1" class="news-title">
                    íŒŒì´ì¬ìœ¼ë¡œ ì—…ë¬´ ìë™í™” ì„±ê³µ ì‚¬ë¡€
                </a>
                <span class="news-date">2024-01-15</span>
                <span class="news-press">í…Œí¬ë‰´ìŠ¤</span>
            </div>
            <div class="news-item">
                <a href="/news/2" class="news-title">
                    ì›¹ ìŠ¤í¬ë ˆì´í•‘ìœ¼ë¡œ ë°ì´í„° ìˆ˜ì§‘í•˜ê¸°
                </a>
                <span class="news-date">2024-01-14</span>
                <span class="news-press">ë°ì´í„°ì €ë„</span>
            </div>
            <div class="news-item">
                <a href="/news/3" class="news-title">
                    BeautifulSoup ì™„ë²½ ê°€ì´ë“œ
                </a>
                <span class="news-date">2024-01-13</span>
                <span class="news-press">ì½”ë”©ë§¤ê±°ì§„</span>
            </div>
        </div>
    </body>
    </html>
    """
    
    def extract_news(html):
        """ë‰´ìŠ¤ ì œëª©, ë§í¬, ë‚ ì§œ, ì–¸ë¡ ì‚¬ ì¶”ì¶œ"""
        soup = BeautifulSoup(html, 'html.parser')
        
        # ëª¨ë“  ë‰´ìŠ¤ í•­ëª© ì°¾ê¸°
        news_items = soup.find_all('div', class_='news-item')
        print(f"ğŸ“Š ì°¾ì€ ë‰´ìŠ¤: {len(news_items)}ê°œ\n")
        
        results = []
        
        for item in news_items:
            # ì œëª©ê³¼ ë§í¬
            title_tag = item.find('a', class_='news-title')
            title = title_tag.text.strip()
            link = title_tag.get('href', '')
            
            # ë‚ ì§œ
            date_tag = item.find('span', class_='news-date')
            date = date_tag.text if date_tag else 'ë‚ ì§œ ì—†ìŒ'
            
            # ì–¸ë¡ ì‚¬
            press_tag = item.find('span', class_='news-press')
            press = press_tag.text if press_tag else 'ì–¸ë¡ ì‚¬ ì—†ìŒ'
            
            results.append({
                'title': title,
                'link': link,
                'date': date,
                'press': press
            })
        
        return results
    
    # ì‹¤í–‰
    news_list = extract_news(sample_html)
    
    print("ğŸ“° ì¶”ì¶œëœ ë‰´ìŠ¤ ëª©ë¡:")
    print("="*70)
    
    for i, news in enumerate(news_list, 1):
        print(f"\n{i}. {news['title']}")
        print(f"   ì–¸ë¡ ì‚¬: {news['press']}")
        print(f"   ë‚ ì§œ: {news['date']}")
        print(f"   ë§í¬: {news['link']}")
    
    print("\n" + "="*70)
    print("\n[ì„¤ëª…]")
    print("ì´ ì˜ˆì œëŠ” 3êµì‹œì—ì„œ ë°°ìš¸ ë‚´ìš©ì˜ ë¯¸ë¦¬ë³´ê¸°ì…ë‹ˆë‹¤:")
    print("1. í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •í™•í•œ ìš”ì†Œ ì°¾ê¸°")
    print("2. êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ")
    print("3. ë”•ì…”ë„ˆë¦¬ë¡œ ë°ì´í„° ì •ë¦¬")
    print("4. ì‹¤ì œ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì—ì„œë„ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì ìš© ê°€ëŠ¥")


# ============================================================================
# ë©”ì¸ ë©”ë‰´
# ============================================================================

def show_menu():
    """ë©”ë‰´ í‘œì‹œ"""
    print("\n" + "="*70)
    print("ì›¹ ìŠ¤í¬ë ˆì´í•‘ ì…ë¬¸ - 2êµì‹œ: BeautifulSoup ê¸°ë³¸ ì‚¬ìš©ë²•")
    print("="*70)
    print("\n[í•™ìŠµ ì„¹ì…˜]")
    print("1. BeautifulSoup ê¸°ë³¸ - HTML íŒŒì‹±")
    print("2. find()ì™€ find_all() - íƒœê·¸ ì°¾ê¸°")
    print("3. ë°ì´í„° ì¶”ì¶œ - í…ìŠ¤íŠ¸ì™€ ì†ì„±")
    print("4. HTML êµ¬ì¡° íƒìƒ‰ - ë¶€ëª¨, ìì‹, í˜•ì œ")
    print("5. ì‹¤ì „ ì˜ˆì œ - ëª¨ë“  ë§í¬ ì¶”ì¶œ")
    print("\n[ì‹¤ìŠµ ë° í‰ê°€]")
    print("6. ì‹¤ìŠµ ë¯¸ì…˜")
    print("7. í€´ì¦ˆ")
    print("8. ì •ë‹µ ë° í•´ì„¤")
    print("9. ì¢…í•© ì‹¤ìŠµ (ë‰´ìŠ¤ ë°ì´í„° ì¶”ì¶œ)")
    print("\n[ì „ì²´ ì‹¤í–‰]")
    print("0. ëª¨ë“  ì„¹ì…˜ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰")
    print("q. ì¢…ë£Œ")
    print("="*70)


def run_all_sections():
    """ëª¨ë“  ì„¹ì…˜ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰"""
    sections = [
        section1_basic_parsing,
        section2_find_methods,
        section3_extract_data,
        section4_navigate_tree,
        section5_real_example,
        practice_missions,
        quiz,
        show_answers,
        comprehensive_example
    ]
    
    for section in sections:
        section()
        print("\n" + "-"*70)
        input("â Enterë¥¼ ëˆŒëŸ¬ ê³„ì†...")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    while True:
        show_menu()
        choice = input("\nì„ íƒí•˜ì„¸ìš” (0-9, q): ").strip().lower()
        
        if choice == 'q':
            print("\nğŸ‘‹ í•™ìŠµì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤!")
            break
        elif choice == '0':
            run_all_sections()
        elif choice == '1':
            section1_basic_parsing()
        elif choice == '2':
            section2_find_methods()
        elif choice == '3':
            section3_extract_data()
        elif choice == '4':
            section4_navigate_tree()
        elif choice == '5':
            section5_real_example()
        elif choice == '6':
            practice_missions()
        elif choice == '7':
            quiz()
        elif choice == '8':
            show_answers()
        elif choice == '9':
            comprehensive_example()
        else:
            print("âš ï¸ ì˜¬ë°”ë¥¸ ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        
        if choice != '0' and choice != 'q':
            input("\nâ Enterë¥¼ ëˆŒëŸ¬ ë©”ë‰´ë¡œ ëŒì•„ê°€ê¸°...")


if __name__ == "__main__":
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                                                                  â•‘
    â•‘         ì›¹ ìŠ¤í¬ë ˆì´í•‘ ì…ë¬¸ - 2êµì‹œ: BeautifulSoup ê¸°ë³¸           â•‘
    â•‘                                                                  â•‘
    â•‘                  HTMLì—ì„œ ì›í•˜ëŠ” ë°ì´í„° ì¶”ì¶œí•˜ê¸°                  â•‘
    â•‘                                                                  â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    print("ğŸ“š í•™ìŠµ ëª©í‘œ:")
    print("  1. HTML íŒŒì‹± ê°œë… ì´í•´")
    print("  2. find()ì™€ find_all()ë¡œ íƒœê·¸ ì°¾ê¸°")
    print("  3. í…ìŠ¤íŠ¸ì™€ ì†ì„± ì¶”ì¶œí•˜ê¸°")
    print("  4. ì‹¤ì „: ì›¹í˜ì´ì§€ì—ì„œ ë§í¬ ì¶”ì¶œ")
    
    main()
